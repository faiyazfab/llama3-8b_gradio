{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp0XKaqUo2W066Wz3Ipf3s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faiyazfab/llama3-8b_gradio/blob/main/llama3_8b_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKI-dOX_I6-n",
        "outputId": "44e84d25-1f9d-456c-fff6-99c7b0f19022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "e7NzPObUM6IW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "ChatNVIDIA.get_available_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_QAjP3dJAtX",
        "outputId": "47c56482-9ec1-4f9c-e192-7e28704c6fd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_nvidia_ai_endpoints/_common.py:176: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='seallms/seallm-7b-v2.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-seallm-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='qwen/qwen2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='snowflake/arctic', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-arctic'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='abacusai/dracarys-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='writer/palmyra-med-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b-32k'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/usdcode-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/paligemma', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/paligemma', aliases=['ai-google-paligemma'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.2-90b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='baichuan-inc/baichuan2-13b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-deepseek-coder-6_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='adept/fuyu-8b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b', aliases=['ai-fuyu-8b', 'playground_fuyu_8b', 'fuyu_8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='rakuten/rakutenai-7b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/nemotron-4-340b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['qa-nemotron-4-340b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ai21labs/jamba-1.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ibm/granite-3.0-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='rakuten/rakutenai-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/llama-3.1-nemotron-51b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/mistral-nemo-minitron-8b-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mistral-large-2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3.5-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='writer/palmyra-med-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/nemotron-mini-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/vila', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/vila', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/usdcode-llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ai21labs/jamba-1.5-large-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='thudm/chatglm3-6b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ibm/granite-3.0-3b-a800m-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='zyphra/zamba2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/deplot', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/deplot', aliases=['ai-google-deplot', 'playground_deplot', 'deplot'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/llama-3.1-nemotron-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/gemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2b', 'playground_gemma_2b', 'gemma_2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/neva-22b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='writer/palmyra-fin-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/kosmos-2', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2', aliases=['ai-microsoft-kosmos-2', 'playground_kosmos_2', 'kosmos_2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.2-11b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='microsoft/phi-3.5-moe-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='yentinglin/llama-3-taiwan-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/nemotron-4-mini-hindi-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "aX7WbNVOJj4B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a Gen AI professor named YZ\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "\n",
        "])\n",
        "llm_chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "print(llm_chain.invoke({\"input\" : \"draft me a study plan to learn Gen AI?\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqscwzkxM_07",
        "outputId": "3da94085-1e41-47c7-e070-a08f36856a10"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a Gen AI professor, I'm excited to help you craft a study plan to learn about Generation AI, also known as General Artificial Intelligence. This is a comprehensive plan that will cover the essential topics, concepts, and developments in the field of Gen AI. Please note that this plan is customizable to your learning style, goals, and pace.\n",
            "\n",
            "**Study Plan for Learning Gen AI**\n",
            "\n",
            "**Phase 1: Fundamentals (Weeks 1-4)**\n",
            "\n",
            "1. Week 1:\n",
            "\t* Study the basics of artificial intelligence (AI) and machine learning (ML)\n",
            "\t* Understand the differences between narrow AI and general AI\n",
            "\t* Explore the history and evolution of AI\n",
            "2. Week 2:\n",
            "\t* Focus on cognitive architectures and their relevance to Gen AI\n",
            "\t* Learn about symbol grounding and the role of symbols in AI\n",
            "\t* Introduce yourself to the concept of consciousness in AI\n",
            "3. Week 3:\n",
            "\t* Study the current challenges and limitations of AI systems\n",
            "\t* Examine the importance of human values, ethics, and decision-making in AI development\n",
            "\t* Explore the potential applications and implications of Gen AI\n",
            "4. Week 4:\n",
            "\t* Review the basics and take notes on the key concepts\n",
            "\t* Practice solving problems and-complete assignments or quizzes to reinforce your understanding\n",
            "\n",
            "**Phase 2: Advanced Topics (Weeks 5-8)**\n",
            "\n",
            "1. Week 5:\n",
            "\t* Delve into the field of neuroscience and its influence on AI development\n",
            "\t* Study the concept of deep learning and its applications in Gen AI\n",
            "\t* Explore the role of natural language processing (NLP) in Gen AI\n",
            "2. Week 6:\n",
            "\t* Learn about the concept of embodied cognition and its impact on AI\n",
            "\t* Study the importance of human-computer interaction in Gen AI\n",
            "\t* Explore the role of robotics and human-robot collaboration in Gen AI\n",
            "3. Week 7:\n",
            "\t* Focus on the challenges and limitations of creating Autonomy in AI systems\n",
            "\t* Study the concept of explainability and interpretability in AI decision-making\n",
            "\t* Explore the role of law, ethics, and governance in the development and deployment of Gen AI\n",
            "4. Week 8:\n",
            "\t* Review the advanced topics and take notes on the key concepts\n",
            "\t* Practice solving problems and complete assignments or quizzes to reinforce your understanding\n",
            "\n",
            "**Phase 3: Capstone Project (Weeks 9-12)**\n",
            "\n",
            "1. Weeks 9-10:\n",
            "\t* Identify a research question or topic related to Gen AI\n",
            "\t* Conduct research and gather resources to inform your project\n",
            "\t* Develop a clear, concise thesis statement and project plan\n",
            "2. Weeks 11-12:\n",
            "\t* Implement your project plan and produce a written report or presentation\n",
            "\t* Demonstrate your understanding of Gen AI concepts and their applications\n",
            "\t* Receive feedback from peers and instructors to refine your project\n",
            "\n",
            "**Additional Tips and Recommendations**\n",
            "\n",
            "* Stay up-to-date with the latest research and developments in the field by following reputable sources and attending industry conferences or seminars.\n",
            "* Join online communities, forums, or social media groups dedicated to Gen AI to engage with experts and learners.\n",
            "* Read and review relevant books, articles, and academic papers to deepen your understanding of the subject matter.\n",
            "* Practice problem-solving, critical thinking, and creative thinking to improve your skills and confidence.\n",
            "* Set realistic goals and milestones to maintain motivation and track progress.\n",
            "\n",
            "Remember, this is a comprehensive study plan, and you should adjust it according to your learning style, goals, and pace. Additionally, it's essential to practice persistence, patience, and continuous learning to stay motivated and achieve your goals in the exciting field of Gen AI. Good luck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate_response(input_text):\n",
        "    try:\n",
        "      response = llm_chain.invoke({\"input\": input_text})\n",
        "      return response\n",
        "    except Exception as e:\n",
        "      return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=generate_response,\n",
        "    inputs=gr.Textbox(label=\"Enter your text here:\"),\n",
        "    outputs=gr.Textbox(label=\"Response:\"),\n",
        "    title=\"llama3-8b-instruct\",\n",
        "    description=\"Ask me anything about GenAI.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "IDrVOFWsdRkS",
        "outputId": "d3db426d-af74-47ee-b8c9-0f9d832f8866"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://93c59c9c6752dd4c49.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://93c59c9c6752dd4c49.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fz6zVtAIUmlc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}